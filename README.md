ğŸš€ Sistema de RecomendaÃ§Ã£o de Produtos com IA
Este projeto Ã© um sistema de recomendaÃ§Ã£o de produtos inteligente, construÃ­do com FastAPI, CrewAI e um Large Language Model (LLM) rodando localmente via Ollama. Ele fornece recomendaÃ§Ãµes personalizadas para usuÃ¡rios, utilizando o poder da inteligÃªncia artificial para entender suas necessidades e sugerir produtos relevantes.

âœ¨ Funcionalidades
RecomendaÃ§Ãµes Personalizadas: SugestÃµes com base nas preferÃªncias do usuÃ¡rio usando um LLM.

Agentes de IA: Uso do CrewAI para orquestraÃ§Ã£o inteligente com agentes autÃ´nomos.

LLM Local: Modelo llama2:7b-chat-q2_K executado via Ollama.

API RESTful: Backend robusto com FastAPI.

AutenticaÃ§Ã£o JWT: SeguranÃ§a via tokens.

Banco de Dados PostgreSQL

Docker Compose: Setup simples e portÃ¡til.

DocumentaÃ§Ã£o Swagger/OpenAPI

Fallback Inteligente: RecomendaÃ§Ãµes padrÃ£o em caso de erro no LLM.

ğŸ—ï¸ Arquitetura

UsuÃ¡rio/Cliente
       â†“
 FastAPI (Web) â”€â”€â”€â†’ CrewAI Agent â”€â”€â”€â†’ Ollama (LLM)
       â†“
  PostgreSQL
FastAPI: Interface REST, autenticaÃ§Ã£o e integraÃ§Ã£o com agentes.

CrewAI: OrquestraÃ§Ã£o de agentes de recomendaÃ§Ã£o via LangChain.

Ollama: ExecuÃ§Ã£o local do modelo LLM.

PostgreSQL: Armazenamento persistente.

Docker Compose: Infraestrutura containerizada.

âš™ï¸ InstalaÃ§Ã£o e ExecuÃ§Ã£o
PrÃ©-requisitos: Docker + Docker Compose

1. Clonar o Projeto

git clone https://github.com/seu-usuario/ia-recommendation.git
cd ia-recommendation

2. Configurar o .env
Crie um arquivo .env na raiz:

env

DATABASE_URL=postgresql://user:password@db:5432/mydatabase
SECRET_KEY=sua_super_chave_secreta
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30
OLLAMA_BASE_URL=http://ollama:11434

3. Subir os ContÃªineres


docker compose up --build -d

4. Baixar o Modelo LLM

docker exec -it ia-recommendation-ollama-1 ollama pull llama2:7b-chat-q2_K

5. Migrar o Banco de Dados

docker exec -it ia-recommendation-web-1 alembic upgrade head

6. Acessar a AplicaÃ§Ã£o
Swagger: http://localhost:8000/docs

ReDoc: http://localhost:8000/redoc

ğŸ” AutenticaÃ§Ã£o

Criar usuÃ¡rio: POST /api/v1/users/

Obter token: POST /api/v1/token

Usar token JWT: clique em "Authorize" no Swagger e adicione Bearer SEU_TOKEN_AQUI.

ğŸ”‘ Endpoints Principais

POST /api/v1/users/ â€“ Cria usuÃ¡rio.

POST /api/v1/token â€“ Gera token de acesso.

GET /api/v1/users/me/ â€“ Retorna dados do usuÃ¡rio.

GET /api/v1/recommendations/ â€“ Lista de recomendaÃ§Ãµes personalizadas.

ğŸ› ï¸ Desafios TÃ©cnicos

1. IntegraÃ§Ã£o com Ollama
Problema: Erros como "modelo nÃ£o encontrado" e "LLM provider not provided".

SoluÃ§Ã£o: Garantir nome exato do modelo, definir OLLAMA_BASE_URL e configurar ChatOllama corretamente no cÃ³digo.

2. OrquestraÃ§Ã£o com Docker
Problema: Ordem de inicializaÃ§Ã£o e dependÃªncias entre serviÃ§os.

SoluÃ§Ã£o: Uso de depends_on, rebuild frequente com --build, logs para debug.

3. Parsing de Respostas do LLM
Problema: Formatos de resposta inconsistentes.

SoluÃ§Ã£o: PydanticOutputParser com esquema bem definido e prompts especÃ­ficos para o formato JSON.

ğŸ¤ ContribuiÃ§Ãµes

Pull requests e issues sÃ£o bem-vindos!

ğŸ“„ LicenÃ§a

DistribuÃ­do sob a licenÃ§a MIT. Veja LICENSE para mais detalhes.

